{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SBERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPayJOolKaQW+k+Mx+cAHlp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tWJnLmrhPyHo"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDbb7eG4eKpm"},"source":["! pip3 install sentence_transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NVOSYbdPLtY"},"source":["from sentence_transformers import SentenceTransformer, models, util, SentencesDataset, InputExample, losses, evaluation\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import transformers\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ew5hvJTmZscy"},"source":["word_embedding_model = models.Transformer('bert-base-chinese', max_seq_length=64)\n","pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n","model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f'{total_params:,} total parameters.')\n","total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'{total_trainable_params:,} training parameters.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wdlOjfJrWxx"},"source":["word_embedding_model.get_word_embedding_dimension()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1hTIdoQdJEQ"},"source":["import pandas as pd\n","import numpy as np\n","\n","data_path='/content/drive/My Drive/Sentence_pair_modeling/LCQMC/data/'\n","train=pd.read_csv(os.path.join(data_path,'train.tsv'),sep='\\t',names=['s1','s2','label'])\n","train_examples = []\n","for i in range(len(train)):\n","  train_examples.append(InputExample(texts=[train.s1[i],train.s2[i]], label=int(train.label[i])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"By0mmqysExWf"},"source":["len(dev)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ybo1pFhxBK2"},"source":["dev=pd.read_csv(os.path.join(data_path,'LCQMC_dev.tsv'),sep='\\t',names=['s1','s2','similarity'])\n","dev_examples = []\n","for i in range(len(dev)):\n","  dev_examples.append(InputExample(texts=[dev.s1[i],dev.s2[i]], label=int(dev.similarity[i])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SjX2Rw2Svidp"},"source":["# Define your train dataset, the dataloader and the train loss\n","\n","train_dataset = SentencesDataset(train_examples, model)\n","train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n","dev_dataset = SentencesDataset(dev_examples, model)\n","dev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=300)\n","train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels = 2)\n","evaluator = evaluation.LabelAccuracyEvaluator(dev_dataloader,softmax_model = train_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ieCHQITvwRSC"},"source":["model_save_path = '/content/drive/My Drive/Sentence_pair_modeling/Models/Sentence-BERT/output_sbert'\n","model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3,  evaluator=evaluator, \n","                  evaluation_steps=500, output_path = model_save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rs4ON7IOUgb6"},"source":["import torch\n","torch.save(train_loss, os.path.join(model_save_path,\"3_Softmax/pytorch_model.bin\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgxRrQuQpNh5"},"source":["import matplotlib.pyplot as plt\n","dev_result=pd.read_csv(os.path.join(model_save_path,'accuracy_evaluation_results.csv'))\n","dev_result.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-YVQIrOWKi8"},"source":["dev_result['steps'] = dev_result['steps'].apply(lambda x: x if x!=-1 else 7462)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zOodehwUn_r"},"source":["scale_ls = range(len(dev_result))\n","index_ls = [dev_result.epoch[i].astype(str) + '-' + dev_result.steps[i].astype(str) for i in range(len(dev_result))]\n","plt.figure(figsize=(16,7))\n","plt.plot(scale_ls[:15], dev_result.accuracy[:15], linewidth=3,label='epoch_0',color='r')\n","plt.plot(scale_ls[14:30], dev_result.accuracy[14:30], linewidth=3,label='epoch_1',color='orange')\n","plt.plot(scale_ls[29:], dev_result.accuracy[29:], linewidth=3,label='epoch_2',color='b')\n","plt.xticks(scale_ls,index_ls, rotation = 90) \n","plt.ylabel('Accuracy',fontsize=12)\n","plt.xlabel('epoch-step',fontsize=12)\n","plt.title('Accuracy on dev dataset per 500 steps',fontsize=16)\n","plt.axvline(x=14,ls=\"--\",c=\"k\",alpha=0.7)\n","plt.axvline(x=29,ls=\"--\",c=\"k\",alpha=0.7)\n","plt.legend(loc='lower right')"],"execution_count":null,"outputs":[]}]}